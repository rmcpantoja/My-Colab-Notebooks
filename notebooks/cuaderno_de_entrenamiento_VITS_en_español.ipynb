{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOSLW0tZUYgxl3A03AcsPv5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rmcpantoja/My-Colab-Notebooks/blob/main/notebooks/cuaderno_de_entrenamiento_VITS_en_espa%C3%B1ol.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cuaderno para entrenamiento de [VITS](https://github.com/jaywalnut310/vits)\n",
        "\n",
        "Cuaderno y soporte para el Español desarrollados por [rmcpantoja](https://github.com/rmcpantoja/), en colaboración con [Mixomo](https://github.com/Mixomo/efficient-vits-finetuning-Spanish-support-WIP-)\n",
        "\n",
        "## créditos:\n",
        "\n",
        "VITS: Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech. (Jaehyeon Kim, Jungil Kong, y Juhee Son, 2021): https://arxiv.org/abs/2106.06103"
      ],
      "metadata": {
        "id": "E1XlFGnCea3e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Primeros pasos"
      ],
      "metadata": {
        "id": "KsSF9JIGfwmu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Comprobar la GPU asignada\n",
        "\n",
        "#@markdown Una GPU con mayor capacidad puede llebar mayor velocidad de respuesta.\n",
        "!nvidia-smi -L"
      ],
      "metadata": {
        "cellView": "form",
        "id": "oeDHAaq2f2lK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Montar Google Drive\n",
        "#@markdown Necesitas como mínimo 1-2 GB de espacio libre.\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "VBqc70UggSGL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Instalar software\n",
        "\n",
        "#@markdown En esta celda se instalará el sintetizador y sus componentes necesarios para ejecutar el entrenamiento. (esto puede tomar un tiempo)\n",
        "\n",
        "#@markdown **Nota: por ahora, el fine-tuning/ajuste de tus datasets se entrenarán usando el modelo Inglés de LJSpeech como base. Estamos en camino para modelos preentrenados en español, por lo que podrá mejorar la capacidad de entrenamiento. Entonces, necesitarás mucha cantidad de datos para que funcione decente.**\n",
        "%cd /content\n",
        "!git clone https://github.com/Mixomo/efficient-vits-finetuning-Spanish-support-WIP- vits\n",
        "%cd vits\n",
        "!cp /usr/local/cuda/lib64/libcudart.so.11.0 /usr/lib64-nvidia/libcudart.so.11.0\n",
        "# FOR ORIGINAL REPO:\n",
        "#!pip install Cython librosa==0.8.0 matplotlib==3.3.1 numpy phonemizer scipy==1.7.2 tensorboard Unidecode torch==1.13.1+cu117 torchvision==0.14.1+cu117 torchaudio==0.13.1 torchtext==0.14.1 torchdata==0.5.1 --extra-index-url https://download.pytorch.org/whl/cu117 -U\n",
        "#FOR FINE-TUNE REPO:\n",
        "!pip install Cython librosa==0.9.1 matplotlib numpy scipy tensorboard unidecode protobuf tqdm phonemizer bitsandbytes wandb num2words\n",
        "%cd monotonic_align\n",
        "!python setup.py build_ext --inplace\n",
        "!sudo apt-get install espeak-ng\n",
        "%cd ..\n",
        "# descargar procesador JSON:\n",
        "!wget https://github.com/mikefarah/yq/releases/download/v4.29.2/yq_linux_amd64.tar.gz\n",
        "!tar -xvf yq_linux_amd64.tar.gz\n",
        "#actualizar gdown y descargar modelos preentrenados (ljspeech inglés, español pronto):\n",
        "!pip install --upgrade gdown\n",
        "import gdown\n",
        "gdown.download(\"https://drive.google.com/file/d/1T-u3OV49W6Lv3bDxh-EA63ALZKHqyy0t/view?usp=sharing\", \"/content/vits/pretrained/generator.pth\", quiet=False, fuzzy=True)\n",
        "gdown.download(\"https://drive.google.com/file/d/118ffn807Eqlu891qbNRQP7O9E0-aMPxM/view?usp=sharing\", \"/content/vits/pretrained/discriminator.pth\", quiet=False, fuzzy=True)\n",
        "# check bitsandbytes\n",
        "!python -m bitsandbytes\n",
        "# for dataset:\n",
        "!mkdir /content/vits/wavs"
      ],
      "metadata": {
        "id": "aBc8-Zp9hYhr",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## entrenamiento"
      ],
      "metadata": {
        "id": "NoxLXHjqlgUc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 1. Extrae el conjunto de datos\n",
        "#@markdown Importante: los audios deberán estar en formato wav, (22050hz, 16-bits, mono), y, para comodidad, enumerados. Ejemplo:\n",
        "\n",
        "#@markdown * 1.wav\n",
        "#@markdown * 2.wav\n",
        "#@markdown * 3.wav\n",
        "#@markdown * .....\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "#@markdown #### Ruta de audios. (estos deberán estar en un archivo comprimido zip, sueltos)\n",
        "wavs_path = \"/content/drive/MyDrive/Fakeyou/aldEnhanced/wavs.zip\" #@param {type:\"string\"}\n",
        "!unzip -q \"{wavs_path}\" -d /content/vits/wavs"
      ],
      "metadata": {
        "cellView": "form",
        "id": "5tA2Vmblh7W9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 2. Subir y procesar la transcripción\n",
        "#@markdown importante: la transcripción significa escribir lo que dice el personaje en cada uno de los audios, y deberá tener la siguiente estructura:\n",
        "\n",
        "#@markdown * wavs/1.wav|Cuando los españoles llegaron a América se encontraron con montañas imponentes, ríos inmensos, aborígenes desconocidos, feroces algunos, muy apacibles otros.\n",
        "#@markdown * wavs/2.wav|Al influjo de los elementos de naturaleza tan grandiosa y de sus extraordinarios habitantes, los conquistadores empezaron a tejer una serie de mitos y leyendas para justificar su existencia.\n",
        "#@markdown * wavs/3.wav|La de las amazonas es una de ellas, y cuenta que hace mucho tiempo, la tribu de los Worisiana se instaló a orillas del poderoso río-mar.\n",
        "#@markdown * ...............\n",
        "\n",
        "#@markdown Y así sucesibamente. Además, la transcripción deberá tener un formato .txt (UTF8 sin bom)\n",
        "\n",
        "from google.colab import files\n",
        "import random\n",
        "%cd /content/vits/filelists\n",
        "!rm /content/vits/filelists/list.txt\n",
        "listfn, length = files.upload().popitem()\n",
        "if listfn != \"list.txt\":\n",
        "  !mv \"$listfn\" list.txt\n",
        "%cd ..\n",
        "print(\"¡Transcripción subida! dibidiendo la lista en entrenamiento y validación...\")\n",
        "with open(\"filelists/list.txt\", encoding=\"utf-8\") as f:\n",
        "  original = f.read().split('\\n')\n",
        "\n",
        "ratio = 0.01\n",
        "def split_data(data, ratio):\n",
        "  train_index = list(range(len(data)))\n",
        "  val_index = []\n",
        "  while len(val_index) < ratio * len(train_index):\n",
        "    val_index.append(train_index.pop(random.randint(0, len(train_index)-1)))\n",
        "  return [data[i] for i in train_index], [data[i] for i in val_index]\n",
        "\n",
        "train, val = split_data(original, ratio)\n",
        "\n",
        "with open(\"filelists/list.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "  f.write('\\n'.join(train))\n",
        "\n",
        "with open(\"filelists/list_val.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "  f.write('\\n'.join(val))\n",
        "print(\"¡Completado! Procesando el texto a fonemas...\")\n",
        "!python preprocess.py --filelists \"filelists/list.txt\" \"filelists/list_val.txt\" --text_cleaners \"spanish_cleaners\"\n",
        "print(\"¡Todo listo!\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "OzmsRRbkjV3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 3. Configuración\n",
        "config_path = \"configs/ljs_base.json\"\n",
        "#@markdown #### Nombre deseado para el modelo:\n",
        "model_name = \"Aldo\" #@param {type:\"string\"}\n",
        "!./yq_linux_amd64 -i '.data.name = \"{model_name}\"' \"{config_path}\"\n",
        "#@markdown ---\n",
        "#@markdown #### carpeta de salida (no se recomienda cambiar):\n",
        "output_path = \"/content/drive/MyDrive/colab/vits\" #@param {type:\"string\"}\n",
        "#@markdown ---\n",
        "#@markdown #### Frecuencia de muestreo (opcional):\n",
        "sample_rate = \"22050\" #@param [\"22050\", \"32000\", \"44100\"]\n",
        "!./yq_linux_amd64 -i '.data.sampling_rate = {sample_rate}' \"{config_path}\"\n",
        "#@markdown ---\n",
        "#@markdown #### épocas de entrenamiento:\n",
        "train_epochs = 200 #@param {type:\"integer\"}\n",
        "!./yq_linux_amd64 -i '.train.epochs = {train_epochs}' \"{config_path}\"\n",
        "#@markdown ---\n",
        "##@markdown #### tasa de aprendizaje (no se recomienda cambiar):\n",
        "#learning_rate = 2e-4\n",
        "#!./yq_linux_amd64 -i '.train.learning_rate = {learning_rate}' \"{config_path}\"\n",
        "##@markdown ---\n",
        "#@markdown #### Tamaño del lote:\n",
        "batch_size = 12 #@param {type:\"integer\"}\n",
        "!./yq_linux_amd64 -i '.train.batch_size = {batch_size}' \"{config_path}\"\n",
        "#@markdown ---\n",
        "#@markdown #### Resumir wandb?\n",
        "wandb_resume = False #@param {type:\"boolean\"}\n",
        "if wandb_resume:\n",
        "    resume = \"true\"\n",
        "else:\n",
        "    resume = \"false\"\n",
        "!./yq_linux_amd64 -i '.train.wandb_resume = {resume}' \"{config_path}\"\n",
        "#@markdown ---\n",
        "# train and val:\n",
        "!./yq_linux_amd64 -i '.data.training_files = \"filelists/list.txt.cleaned\"' \"{config_path}\"\n",
        "!./yq_linux_amd64 -i '.data.validation_files = \"filelists/list_val.txt.cleaned\"' \"{config_path}\"\n",
        "# cleaner:\n",
        "!./yq_linux_amd64 -i '.data.text_cleaners = [\"spanish_cleaners\"]' \"{config_path}\""
      ],
      "metadata": {
        "cellView": "form",
        "id": "c7VEnSBqwzJ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 4. Ejecutar la extensión TensorBoard\n",
        "import os\n",
        "if not os.path.exists(\"/content/drive/MyDrive/colab/vits\"):\n",
        "    os.makedirs(\"/content/drive/MyDrive/colab/vits\")\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir \"{output_path}\""
      ],
      "metadata": {
        "cellView": "form",
        "id": "FDKthiBqlkXl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 5. Comenzar el entrenamiento\n",
        "#@markdown Importante: si entrenas desde cero, recuerda eliminar los respaldos anteriores de modelos (si se encuentran) para evitar mayor consumo de espacio.\n",
        "\n",
        "!python train.py --config \"{config_path}\" --model \"{model_name}\" --path \"{output_path}\""
      ],
      "metadata": {
        "cellView": "form",
        "id": "MzzezyVbl2mj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ¿Curioso por escuchar cómo suena el modelo?\n",
        "\n",
        "¡Prueba el cuaderno de síntesis [aquí](https://colab.research.google.com/drive/1mlqngf8t6NUMAD35A8dimMEe2_sXdYAL)!\n",
        "\n",
        "# Cuadernos relacionados/de interés\n",
        "\n",
        "* [Cuaderno completo de SoftVC VITS Singing Voice Conversion (entrenamiento+inferencia)](https://colab.research.google.com/github/rmcpantoja/My-Colab-Notebooks/blob/main/notebooks/Cuaderno_completo_So_Vits_SVC_en_espa%C3%B1ol.ipynb)"
      ],
      "metadata": {
        "id": "7TH_6HQAZbMQ"
      }
    }
  ]
}